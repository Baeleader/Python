{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:00.529415Z",
     "start_time": "2020-02-06T02:25:00.518443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:12.219140Z",
     "start_time": "2020-02-06T02:25:12.174690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant(\"Hello, TensorFlow\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:13.879810Z",
     "start_time": "2020-02-06T02:25:13.870358Z"
    }
   },
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:14.610620Z",
     "start_time": "2020-02-06T02:25:14.606697Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_2:0\", shape=(), dtype=float32) node2 : Tensor(\"Const_3:0\", shape=(), dtype=float32)\n",
      "node3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1:\", node1,\"node2 :\", node2 )\n",
    "print(\"node3: \", node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:15.343810Z",
     "start_time": "2020-02-06T02:25:15.327027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2): [3.0, 4.0]\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"sess.run(node1, node2):\", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build graph using TensorFlow operations\n",
    "2. feed data and run graph (operation) sess.run (op)\n",
    "3. update variables in the graph (and return values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:19.445793Z",
     "start_time": "2020-02-06T02:25:19.430463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a : 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build graph using TesorFlow operations\n",
    "2. feed data and run graph (operation) sess.run(op, feed_dict={x:x_data})\n",
    "3. update variables in the graph (and return values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:21.214392Z",
     "start_time": "2020-02-06T02:25:21.198246Z"
    }
   },
   "outputs": [],
   "source": [
    "# H(x) = Wx + b\n",
    "import tensorflow as tf\n",
    "\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight') # trainable\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#our hypothesis XW+b\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:21.998397Z",
     "start_time": "2020-02-06T02:25:21.990076Z"
    }
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:25:24.519831Z",
     "start_time": "2020-02-06T02:25:24.486733Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost) # 노드 생성 트레인 실행시켜야 코스트를 미니마이즈 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:28:49.566603Z",
     "start_time": "2020-02-06T02:28:48.665889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.334533 [0.07782293] [-1.1166579]\n",
      "20 0.13040823 [1.1369774] [-0.6173201]\n",
      "40 0.042755608 [1.2272654] [-0.54575884]\n",
      "60 0.03814513 [1.2257949] [-0.5160586]\n",
      "80 0.034637805 [1.2160603] [-0.49142]\n",
      "100 0.03145856 [1.20599] [-0.46828836]\n",
      "120 0.028571153 [1.1963171] [-0.4462771]\n",
      "140 0.025948793 [1.1870916] [-0.42530343]\n",
      "160 0.023567073 [1.178299] [-0.40531555]\n",
      "180 0.021404015 [1.1699196] [-0.38626716]\n",
      "200 0.019439468 [1.1619341] [-0.36811405]\n",
      "220 0.017655244 [1.1543237] [-0.35081407]\n",
      "240 0.016034758 [1.1470711] [-0.334327]\n",
      "260 0.014563031 [1.1401592] [-0.3186149]\n",
      "280 0.013226378 [1.1335722] [-0.30364114]\n",
      "300 0.012012396 [1.1272948] [-0.28937107]\n",
      "320 0.010909854 [1.1213125] [-0.2757717]\n",
      "340 0.009908511 [1.1156112] [-0.26281148]\n",
      "360 0.008999064 [1.110178] [-0.25046033]\n",
      "380 0.008173097 [1.105] [-0.23868965]\n",
      "400 0.007422944 [1.1000655] [-0.22747226]\n",
      "420 0.006741654 [1.0953628] [-0.21678194]\n",
      "440 0.006122867 [1.0908811] [-0.20659402]\n",
      "460 0.0055608875 [1.08661] [-0.19688483]\n",
      "480 0.005050486 [1.0825397] [-0.18763196]\n",
      "500 0.0045869164 [1.0786604] [-0.17881386]\n",
      "520 0.0041659256 [1.0749638] [-0.17041025]\n",
      "540 0.0037835569 [1.0714407] [-0.16240165]\n",
      "560 0.0034362879 [1.0680833] [-0.15476929]\n",
      "580 0.0031208852 [1.0648836] [-0.14749572]\n",
      "600 0.0028344465 [1.0618345] [-0.140564]\n",
      "620 0.0025742855 [1.0589284] [-0.13395807]\n",
      "640 0.0023380138 [1.056159] [-0.12766252]\n",
      "660 0.0021234178 [1.0535197] [-0.12166285]\n",
      "680 0.0019285224 [1.0510044] [-0.11594515]\n",
      "700 0.0017515129 [1.0486073] [-0.11049609]\n",
      "720 0.0015907488 [1.0463231] [-0.10530318]\n",
      "740 0.0014447438 [1.0441461] [-0.10035432]\n",
      "760 0.0013121438 [1.0420713] [-0.09563807]\n",
      "780 0.0011917114 [1.0400943] [-0.09114358]\n",
      "800 0.001082328 [1.0382099] [-0.08686008]\n",
      "820 0.0009829897 [1.0364141] [-0.08277795]\n",
      "840 0.0008927663 [1.0347028] [-0.07888771]\n",
      "860 0.0008108231 [1.0330719] [-0.07518026]\n",
      "880 0.0007364072 [1.0315176] [-0.07164703]\n",
      "900 0.00066880876 [1.0300363] [-0.06827988]\n",
      "920 0.0006074263 [1.0286248] [-0.06507092]\n",
      "940 0.0005516742 [1.0272796] [-0.06201287]\n",
      "960 0.00050103944 [1.0259975] [-0.05909853]\n",
      "980 0.00045505198 [1.0247757] [-0.05632111]\n",
      "1000 0.0004132851 [1.0236114] [-0.05367423]\n",
      "1020 0.00037535265 [1.0225018] [-0.05115177]\n",
      "1040 0.00034090472 [1.0214443] [-0.04874787]\n",
      "1060 0.0003096203 [1.020437] [-0.04645715]\n",
      "1080 0.000281201 [1.019476] [-0.04427401]\n",
      "1100 0.00025538853 [1.0185608] [-0.04219318]\n",
      "1120 0.00023194811 [1.0176885] [-0.0402102]\n",
      "1140 0.00021065772 [1.0168571] [-0.03832043]\n",
      "1160 0.00019132283 [1.0160649] [-0.03651949]\n",
      "1180 0.00017376302 [1.01531] [-0.03480318]\n",
      "1200 0.00015781446 [1.0145904] [-0.03316756]\n",
      "1220 0.00014332974 [1.0139048] [-0.0316088]\n",
      "1240 0.00013017374 [1.0132513] [-0.03012337]\n",
      "1260 0.00011822622 [1.0126286] [-0.02870768]\n",
      "1280 0.00010737564 [1.0120351] [-0.02735853]\n",
      "1300 9.75214e-05 [1.0114695] [-0.0260728]\n",
      "1320 8.856838e-05 [1.0109304] [-0.02484747]\n",
      "1340 8.0440455e-05 [1.0104166] [-0.02367973]\n",
      "1360 7.3056224e-05 [1.009927] [-0.02256681]\n",
      "1380 6.635074e-05 [1.0094606] [-0.02150619]\n",
      "1400 6.025989e-05 [1.0090159] [-0.02049543]\n",
      "1420 5.4730044e-05 [1.0085922] [-0.0195322]\n",
      "1440 4.9706472e-05 [1.0081885] [-0.01861427]\n",
      "1460 4.5144112e-05 [1.0078036] [-0.01773947]\n",
      "1480 4.1000996e-05 [1.0074369] [-0.01690576]\n",
      "1500 3.7237416e-05 [1.0070875] [-0.0161113]\n",
      "1520 3.3819026e-05 [1.0067543] [-0.01535412]\n",
      "1540 3.0715233e-05 [1.0064368] [-0.01463248]\n",
      "1560 2.7895621e-05 [1.0061344] [-0.0139448]\n",
      "1580 2.53362e-05 [1.0058461] [-0.01328947]\n",
      "1600 2.301029e-05 [1.0055712] [-0.01266491]\n",
      "1620 2.089877e-05 [1.0053096] [-0.01206975]\n",
      "1640 1.8980334e-05 [1.00506] [-0.01150251]\n",
      "1660 1.7238113e-05 [1.0048221] [-0.01096189]\n",
      "1680 1.5655767e-05 [1.0045955] [-0.0104467]\n",
      "1700 1.4219188e-05 [1.0043795] [-0.00995576]\n",
      "1720 1.2913903e-05 [1.0041738] [-0.00948789]\n",
      "1740 1.1728461e-05 [1.0039775] [-0.00904198]\n",
      "1760 1.0652305e-05 [1.0037906] [-0.00861704]\n",
      "1780 9.674465e-06 [1.0036125] [-0.00821208]\n",
      "1800 8.786655e-06 [1.0034428] [-0.00782618]\n",
      "1820 7.980069e-06 [1.003281] [-0.00745843]\n",
      "1840 7.2476905e-06 [1.0031269] [-0.0071079]\n",
      "1860 6.5827103e-06 [1.0029799] [-0.0067739]\n",
      "1880 5.9783756e-06 [1.0028398] [-0.00645558]\n",
      "1900 5.4296966e-06 [1.0027064] [-0.00615221]\n",
      "1920 4.931398e-06 [1.0025792] [-0.00586311]\n",
      "1940 4.4787034e-06 [1.002458] [-0.00558759]\n",
      "1960 4.0676973e-06 [1.0023426] [-0.00532503]\n",
      "1980 3.6947883e-06 [1.0022326] [-0.00507484]\n",
      "2000 3.3556673e-06 [1.0021276] [-0.0048364]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the Line\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:33:16.871777Z",
     "start_time": "2020-02-06T02:33:16.138834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.509334 [-0.2927314] [-0.13395439]\n",
      "20 0.094264366 [0.73946595] [0.29968074]\n",
      "40 0.016396629 [0.84421027] [0.32628784]\n",
      "60 0.014264122 [0.86033964] [0.3148281]\n",
      "80 0.012949211 [0.8677419] [0.30040124]\n",
      "100 0.011760627 [0.8740373] [0.28631857]\n",
      "120 0.010681192 [0.87996477] [0.272866]\n",
      "140 0.009700822 [0.8856067] [0.2600426]\n",
      "160 0.00881043 [0.89098287] [0.2478215]\n",
      "180 0.00800177 [0.8961063] [0.2361748]\n",
      "200 0.00726735 [0.90098894] [0.22507551]\n",
      "220 0.006600322 [0.90564203] [0.21449777]\n",
      "240 0.005994515 [0.91007656] [0.20441715]\n",
      "260 0.00544431 [0.91430265] [0.19481027]\n",
      "280 0.004944619 [0.91833013] [0.18565491]\n",
      "300 0.0044907746 [0.9221683] [0.1769298]\n",
      "320 0.0040785917 [0.9258261] [0.16861475]\n",
      "340 0.0037042473 [0.929312] [0.1606905]\n",
      "360 0.0033642622 [0.93263406] [0.15313865]\n",
      "380 0.0030554694 [0.9358] [0.1459417]\n",
      "400 0.0027750286 [0.93881714] [0.13908297]\n",
      "420 0.0025203205 [0.9416926] [0.13254656]\n",
      "440 0.0022889976 [0.94443274] [0.1263174]\n",
      "460 0.0020789069 [0.9470442] [0.12038098]\n",
      "480 0.0018880988 [0.9495328] [0.11472356]\n",
      "500 0.0017148023 [0.95190465] [0.10933198]\n",
      "520 0.0015574124 [0.954165] [0.10419379]\n",
      "540 0.0014144691 [0.95631903] [0.09929706]\n",
      "560 0.0012846432 [0.9583718] [0.09463048]\n",
      "580 0.0011667302 [0.9603283] [0.09018321]\n",
      "600 0.001059641 [0.96219265] [0.08594491]\n",
      "620 0.00096238166 [0.9639695] [0.0819058]\n",
      "640 0.00087405083 [0.9656628] [0.07805652]\n",
      "660 0.00079382636 [0.9672765] [0.07438815]\n",
      "680 0.00072097016 [0.9688144] [0.07089221]\n",
      "700 0.00065479503 [0.97028005] [0.06756046]\n",
      "720 0.0005946917 [0.9716768] [0.06438535]\n",
      "740 0.00054011226 [0.97300786] [0.06135948]\n",
      "760 0.00049053994 [0.9742764] [0.0584758]\n",
      "780 0.00044551364 [0.97548527] [0.05572765]\n",
      "800 0.00040462322 [0.97663736] [0.05310867]\n",
      "820 0.00036748507 [0.97773534] [0.05061276]\n",
      "840 0.000333756 [0.97878164] [0.04823418]\n",
      "860 0.00030312134 [0.9797789] [0.04596737]\n",
      "880 0.0002752994 [0.9807293] [0.04380703]\n",
      "900 0.00025003264 [0.98163486] [0.04174826]\n",
      "920 0.00022708236 [0.982498] [0.03978623]\n",
      "940 0.00020624115 [0.98332053] [0.03791643]\n",
      "960 0.00018731084 [0.9841044] [0.03613449]\n",
      "980 0.00017011992 [0.98485136] [0.03443633]\n",
      "1000 0.00015450465 [0.98556334] [0.03281794]\n",
      "1020 0.00014032431 [0.9862418] [0.03127562]\n",
      "1040 0.00012744487 [0.98688835] [0.02980578]\n",
      "1060 0.000115747156 [0.98750454] [0.02840504]\n",
      "1080 0.00010512484 [0.9880918] [0.02707011]\n",
      "1100 9.547547e-05 [0.98865145] [0.02579791]\n",
      "1120 8.671236e-05 [0.98918474] [0.0245855]\n",
      "1140 7.875289e-05 [0.98969316] [0.0234301]\n",
      "1160 7.152376e-05 [0.99017763] [0.02232886]\n",
      "1180 6.495945e-05 [0.99063915] [0.02127943]\n",
      "1200 5.8996015e-05 [0.9910791] [0.02027934]\n",
      "1220 5.3582015e-05 [0.9914984] [0.01932626]\n",
      "1240 4.8663485e-05 [0.99189794] [0.01841799]\n",
      "1260 4.4196368e-05 [0.9922787] [0.01755239]\n",
      "1280 4.01397e-05 [0.99264157] [0.01672749]\n",
      "1300 3.6456135e-05 [0.9929874] [0.01594135]\n",
      "1320 3.3109663e-05 [0.99331695] [0.01519217]\n",
      "1340 3.0070718e-05 [0.993631] [0.01447819]\n",
      "1360 2.7310938e-05 [0.99393034] [0.01379777]\n",
      "1380 2.4804287e-05 [0.9942156] [0.01314933]\n",
      "1400 2.2527684e-05 [0.9944874] [0.01253137]\n",
      "1420 2.0460064e-05 [0.9947465] [0.01194244]\n",
      "1440 1.8582285e-05 [0.9949933] [0.01138122]\n",
      "1460 1.6876485e-05 [0.9952287] [0.01084634]\n",
      "1480 1.5327225e-05 [0.99545294] [0.01033659]\n",
      "1500 1.3920783e-05 [0.9956666] [0.00985081]\n",
      "1520 1.2643172e-05 [0.9958703] [0.00938784]\n",
      "1540 1.1482435e-05 [0.99606436] [0.00894662]\n",
      "1560 1.0428793e-05 [0.9962494] [0.00852616]\n",
      "1580 9.471199e-06 [0.9964256] [0.00812546]\n",
      "1600 8.602292e-06 [0.99659353] [0.00774362]\n",
      "1620 7.812625e-06 [0.99675363] [0.00737971]\n",
      "1640 7.0958463e-06 [0.9969062] [0.00703289]\n",
      "1660 6.4442356e-06 [0.99705166] [0.00670237]\n",
      "1680 5.852636e-06 [0.99719024] [0.00638736]\n",
      "1700 5.315564e-06 [0.9973222] [0.00608718]\n",
      "1720 4.8276165e-06 [0.997448] [0.00580113]\n",
      "1740 4.3847103e-06 [0.99756795] [0.00552852]\n",
      "1760 3.9822808e-06 [0.9976823] [0.00526872]\n",
      "1780 3.6168813e-06 [0.9977912] [0.00502112]\n",
      "1800 3.2849537e-06 [0.99789494] [0.00478517]\n",
      "1820 2.983507e-06 [0.9979939] [0.00456032]\n",
      "1840 2.7097287e-06 [0.9980882] [0.00434601]\n",
      "1860 2.4607223e-06 [0.99817806] [0.00414176]\n",
      "1880 2.2349925e-06 [0.99826366] [0.00394712]\n",
      "1900 2.0300567e-06 [0.99834526] [0.00376161]\n",
      "1920 1.8435109e-06 [0.99842304] [0.00358483]\n",
      "1940 1.6743707e-06 [0.9984971] [0.00341637]\n",
      "1960 1.5206446e-06 [0.9985677] [0.00325583]\n",
      "1980 1.3810968e-06 [0.99863505] [0.00310282]\n",
      "2000 1.2542914e-06 [0.99869925] [0.002957]\n"
     ]
    }
   ],
   "source": [
    "# Full 소스 코드\n",
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bise')\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "#minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the Line\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:38:54.334221Z",
     "start_time": "2020-02-06T02:38:52.436941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.25341395 [1.3172162] [-0.26481915]\n",
      "20 0.017175142 [1.1580485] [-0.31583372]\n",
      "40 0.013667725 [1.1368845] [-0.3070334]\n",
      "60 0.012395714 [1.1291434] [-0.29317933]\n",
      "80 0.011257845 [1.1229496] [-0.27945584]\n",
      "100 0.010224551 [1.1171595] [-0.26632762]\n",
      "120 0.0092860935 [1.1116523] [-0.2538116]\n",
      "140 0.00843379 [1.106405] [-0.24188341]\n",
      "160 0.007659692 [1.1014041] [-0.23051575]\n",
      "180 0.006956653 [1.0966387] [-0.21968232]\n",
      "200 0.006318132 [1.0920969] [-0.2093581]\n",
      "220 0.0057382383 [1.0877688] [-0.19951901]\n",
      "240 0.0052115615 [1.083644] [-0.1901424]\n",
      "260 0.0047332137 [1.0797129] [-0.18120632]\n",
      "280 0.004298778 [1.0759667] [-0.17269024]\n",
      "300 0.0039042188 [1.0723966] [-0.16457443]\n",
      "320 0.0035458785 [1.0689942] [-0.15684007]\n",
      "340 0.00322042 [1.0657518] [-0.14946917]\n",
      "360 0.002924839 [1.0626616] [-0.1424447]\n",
      "380 0.0026563844 [1.0597166] [-0.13575025]\n",
      "400 0.0024125732 [1.0569103] [-0.12937042]\n",
      "420 0.0021911324 [1.0542357] [-0.12329046]\n",
      "440 0.0019900205 [1.0516869] [-0.11749627]\n",
      "460 0.0018073715 [1.0492576] [-0.11197441]\n",
      "480 0.0016414867 [1.0469428] [-0.10671206]\n",
      "500 0.0014908224 [1.0447366] [-0.10169698]\n",
      "520 0.0013539927 [1.0426342] [-0.09691759]\n",
      "540 0.001229718 [1.0406308] [-0.0923629]\n",
      "560 0.001116847 [1.038721] [-0.08802212]\n",
      "580 0.001014334 [1.0369012] [-0.08388537]\n",
      "600 0.00092123955 [1.0351671] [-0.07994304]\n",
      "620 0.00083668204 [1.0335144] [-0.07618599]\n",
      "640 0.0007598896 [1.0319393] [-0.07260552]\n",
      "660 0.00069014047 [1.0304384] [-0.0691934]\n",
      "680 0.0006267996 [1.0290079] [-0.06594162]\n",
      "700 0.0005692707 [1.0276445] [-0.06284262]\n",
      "720 0.0005170206 [1.0263454] [-0.05988927]\n",
      "740 0.00046956408 [1.0251073] [-0.05707472]\n",
      "760 0.00042647086 [1.0239273] [-0.05439246]\n",
      "780 0.0003873261 [1.022803] [-0.05183623]\n",
      "800 0.00035177427 [1.0217314] [-0.04940015]\n",
      "820 0.00031948974 [1.0207098] [-0.0470786]\n",
      "840 0.00029016074 [1.0197363] [-0.04486593]\n",
      "860 0.00026352968 [1.0188088] [-0.04275726]\n",
      "880 0.00023933961 [1.0179249] [-0.04074774]\n",
      "900 0.00021737133 [1.0170826] [-0.03883269]\n",
      "920 0.00019742093 [1.0162797] [-0.03700771]\n",
      "940 0.00017930089 [1.0155146] [-0.03526844]\n",
      "960 0.00016284385 [1.0147854] [-0.03361094]\n",
      "980 0.00014789682 [1.0140907] [-0.03203132]\n",
      "1000 0.00013432262 [1.0134286] [-0.03052599]\n",
      "1020 0.00012199388 [1.0127974] [-0.02909141]\n",
      "1040 0.00011079733 [1.012196] [-0.02772424]\n",
      "1060 0.00010062791 [1.0116228] [-0.0264213]\n",
      "1080 9.13934e-05 [1.0110766] [-0.02517962]\n",
      "1100 8.300412e-05 [1.0105561] [-0.02399628]\n",
      "1120 7.538676e-05 [1.0100601] [-0.02286866]\n",
      "1140 6.846597e-05 [1.009587] [-0.02179391]\n",
      "1160 6.2182786e-05 [1.0091366] [-0.02076965]\n",
      "1180 5.647566e-05 [1.0087073] [-0.01979356]\n",
      "1200 5.1291743e-05 [1.0082979] [-0.01886332]\n",
      "1220 4.658394e-05 [1.007908] [-0.0179768]\n",
      "1240 4.2307947e-05 [1.0075364] [-0.01713196]\n",
      "1260 3.8424772e-05 [1.0071822] [-0.01632681]\n",
      "1280 3.489808e-05 [1.0068448] [-0.01555954]\n",
      "1300 3.1695392e-05 [1.006523] [-0.01482835]\n",
      "1320 2.8786917e-05 [1.0062164] [-0.01413145]\n",
      "1340 2.614414e-05 [1.0059242] [-0.01346732]\n",
      "1360 2.3743973e-05 [1.0056459] [-0.0128344]\n",
      "1380 2.1564954e-05 [1.0053805] [-0.01223123]\n",
      "1400 1.9585426e-05 [1.0051278] [-0.01165647]\n",
      "1420 1.7788361e-05 [1.0048867] [-0.01110874]\n",
      "1440 1.6155958e-05 [1.0046571] [-0.01058667]\n",
      "1460 1.4673387e-05 [1.0044383] [-0.01008917]\n",
      "1480 1.3326021e-05 [1.0042297] [-0.00961502]\n",
      "1500 1.2102927e-05 [1.004031] [-0.00916317]\n",
      "1520 1.0992449e-05 [1.0038414] [-0.00873255]\n",
      "1540 9.983308e-06 [1.0036609] [-0.00832214]\n",
      "1560 9.067397e-06 [1.0034888] [-0.00793103]\n",
      "1580 8.234828e-06 [1.003325] [-0.00755829]\n",
      "1600 7.4792297e-06 [1.0031687] [-0.00720312]\n",
      "1620 6.7928218e-06 [1.0030198] [-0.00686463]\n",
      "1640 6.169553e-06 [1.002878] [-0.00654205]\n",
      "1660 5.603137e-06 [1.0027426] [-0.00623465]\n",
      "1680 5.088874e-06 [1.0026138] [-0.00594166]\n",
      "1700 4.6219943e-06 [1.002491] [-0.00566245]\n",
      "1720 4.1976e-06 [1.0023739] [-0.00539637]\n",
      "1740 3.8125593e-06 [1.0022624] [-0.00514278]\n",
      "1760 3.4627913e-06 [1.002156] [-0.00490116]\n",
      "1780 3.1449817e-06 [1.0020547] [-0.00467088]\n",
      "1800 2.85618e-06 [1.0019581] [-0.0044514]\n",
      "1820 2.5943937e-06 [1.0018662] [-0.00424226]\n",
      "1840 2.356255e-06 [1.0017786] [-0.00404293]\n",
      "1860 2.1401727e-06 [1.001695] [-0.00385296]\n",
      "1880 1.9435383e-06 [1.0016153] [-0.00367194]\n",
      "1900 1.7652974e-06 [1.0015396] [-0.00349945]\n",
      "1920 1.6034192e-06 [1.0014671] [-0.00333506]\n",
      "1940 1.4563038e-06 [1.0013983] [-0.0031784]\n",
      "1960 1.3225807e-06 [1.0013326] [-0.00302911]\n",
      "1980 1.201268e-06 [1.0012699] [-0.00288681]\n",
      "2000 1.0910785e-06 [1.0012102] [-0.0027512]\n"
     ]
    }
   ],
   "source": [
    "# PlaceHolders\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "#x_train = [1, 2, 3]\n",
    "#y_train = [1, 2, 3]\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bise')\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "#minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the Line\n",
    "for step in range(2001):\n",
    "    #sess.run(train)\n",
    "    cost_val, W_val, b_val, _=\\\n",
    "        sess.run([cost, W, b, train],\n",
    "                feed_dict = {X:[1,2,3],Y:[1,2,3]})\n",
    "    if step % 20 == 0:\n",
    "        #print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
    "        print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:43:08.827715Z",
     "start_time": "2020-02-06T02:43:07.058284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 151.41434 [-1.9506824] [1.5303649]\n",
      "20 0.21796922 [0.68818486] [2.1795788]\n",
      "40 0.187661 [0.7196521] [2.111936]\n",
      "60 0.16388577 [0.7380621] [2.045678]\n",
      "80 0.1431228 [0.7552168] [1.983745]\n",
      "100 0.124990225 [0.7712478] [1.9258679]\n",
      "120 0.109154835 [0.786229] [1.8717811]\n",
      "140 0.09532583 [0.800229] [1.8212366]\n",
      "160 0.08324878 [0.8133121] [1.7740024]\n",
      "180 0.072701715 [0.82553846] [1.7298615]\n",
      "200 0.06349103 [0.836964] [1.6886115]\n",
      "220 0.05544721 [0.8476414] [1.6500628]\n",
      "240 0.048422378 [0.85761946] [1.6140387]\n",
      "260 0.0422877 [0.8669441] [1.580374]\n",
      "280 0.036930174 [0.875658] [1.5489141]\n",
      "300 0.032251365 [0.8838013] [1.5195142]\n",
      "320 0.028165406 [0.89141124] [1.4920398]\n",
      "340 0.024597038 [0.8985228] [1.466365]\n",
      "360 0.021480728 [0.9051686] [1.4423712]\n",
      "380 0.0187593 [0.9113793] [1.4199489]\n",
      "400 0.016382616 [0.91718316] [1.3989949]\n",
      "420 0.014307035 [0.92260695] [1.3794135]\n",
      "440 0.012494437 [0.9276755] [1.3611143]\n",
      "460 0.010911448 [0.93241215] [1.3440133]\n",
      "480 0.009529027 [0.93683857] [1.3280327]\n",
      "500 0.008321782 [0.9409751] [1.3130985]\n",
      "520 0.007267476 [0.94484067] [1.2991426]\n",
      "540 0.0063467547 [0.9484531] [1.2861007]\n",
      "560 0.0055426536 [0.9518289] [1.2739128]\n",
      "580 0.0048404518 [0.9549837] [1.2625232]\n",
      "600 0.004227206 [0.9579318] [1.2518795]\n",
      "620 0.0036916398 [0.96068686] [1.2419327]\n",
      "640 0.0032239663 [0.9632614] [1.2326376]\n",
      "660 0.002815514 [0.9656675] [1.2239512]\n",
      "680 0.0024588034 [0.9679159] [1.2158335]\n",
      "700 0.0021472904 [0.97001714] [1.2082475]\n",
      "720 0.0018752379 [0.97198075] [1.2011582]\n",
      "740 0.0016376671 [0.97381574] [1.1945335]\n",
      "760 0.0014301963 [0.9755305] [1.1883425]\n",
      "780 0.0012489967 [0.9771331] [1.1825569]\n",
      "800 0.0010907593 [0.9786306] [1.1771501]\n",
      "820 0.00095256616 [0.9800302] [1.1720973]\n",
      "840 0.0008318882 [0.981338] [1.1673757]\n",
      "860 0.0007264914 [0.9825602] [1.1629632]\n",
      "880 0.00063444214 [0.98370236] [1.1588395]\n",
      "900 0.0005540638 [0.98476976] [1.1549859]\n",
      "920 0.00048387266 [0.9857672] [1.1513851]\n",
      "940 0.00042256614 [0.9866993] [1.1480198]\n",
      "960 0.00036902726 [0.98757035] [1.1448748]\n",
      "980 0.0003222756 [0.9883845] [1.1419358]\n",
      "1000 0.00028144923 [0.98914504] [1.1391896]\n",
      "1020 0.00024578965 [0.98985595] [1.1366231]\n",
      "1040 0.00021465038 [0.9905203] [1.1342245]\n",
      "1060 0.00018745629 [0.99114114] [1.1319833]\n",
      "1080 0.00016370791 [0.99172133] [1.1298887]\n",
      "1100 0.0001429677 [0.99226344] [1.1279312]\n",
      "1120 0.00012485454 [0.99277014] [1.126102]\n",
      "1140 0.000109035675 [0.9932436] [1.1243926]\n",
      "1160 9.522216e-05 [0.99368614] [1.122795]\n",
      "1180 8.31597e-05 [0.9940996] [1.1213022]\n",
      "1200 7.2626324e-05 [0.994486] [1.1199074]\n",
      "1220 6.3424595e-05 [0.99484706] [1.1186037]\n",
      "1240 5.5388467e-05 [0.9951846] [1.1173851]\n",
      "1260 4.8370737e-05 [0.9954999] [1.1162467]\n",
      "1280 4.2241885e-05 [0.9957946] [1.1151826]\n",
      "1300 3.689222e-05 [0.99607] [1.1141883]\n",
      "1320 3.2218475e-05 [0.9963274] [1.1132592]\n",
      "1340 2.8136361e-05 [0.9965679] [1.1123908]\n",
      "1360 2.4570947e-05 [0.99679273] [1.1115792]\n",
      "1380 2.1458536e-05 [0.9970028] [1.110821]\n",
      "1400 1.8737599e-05 [0.9971992] [1.1101117]\n",
      "1420 1.6363094e-05 [0.99738264] [1.1094494]\n",
      "1440 1.4289399e-05 [0.99755406] [1.1088306]\n",
      "1460 1.2480139e-05 [0.9977142] [1.1082523]\n",
      "1480 1.0899337e-05 [0.9978639] [1.1077119]\n",
      "1500 9.518197e-06 [0.99800384] [1.1072068]\n",
      "1520 8.312357e-06 [0.9981345] [1.1067348]\n",
      "1540 7.259153e-06 [0.9982567] [1.1062937]\n",
      "1560 6.339753e-06 [0.9983709] [1.1058816]\n",
      "1580 5.5365044e-06 [0.9984776] [1.1054964]\n",
      "1600 4.8353286e-06 [0.9985773] [1.1051364]\n",
      "1620 4.222363e-06 [0.99867046] [1.1048]\n",
      "1640 3.6871897e-06 [0.9987576] [1.1044855]\n",
      "1660 3.2198054e-06 [0.9988389] [1.1041918]\n",
      "1680 2.812267e-06 [0.99891496] [1.1039174]\n",
      "1700 2.456091e-06 [0.998986] [1.1036607]\n",
      "1720 2.1451044e-06 [0.99905235] [1.1034211]\n",
      "1740 1.8732153e-06 [0.9991144] [1.1031971]\n",
      "1760 1.6362164e-06 [0.9991724] [1.1029878]\n",
      "1780 1.428601e-06 [0.9992266] [1.1027921]\n",
      "1800 1.2476496e-06 [0.99927723] [1.1026093]\n",
      "1820 1.0896001e-06 [0.99932456] [1.1024383]\n",
      "1840 9.5170145e-07 [0.99936885] [1.1022786]\n",
      "1860 8.310764e-07 [0.9994102] [1.1021293]\n",
      "1880 7.25598e-07 [0.9994488] [1.1019899]\n",
      "1900 6.3379065e-07 [0.9994849] [1.1018596]\n",
      "1920 5.533591e-07 [0.99951863] [1.1017377]\n",
      "1940 4.8336966e-07 [0.99955016] [1.1016241]\n",
      "1960 4.2222632e-07 [0.9995794] [1.1015179]\n",
      "1980 3.687128e-07 [0.999607] [1.1014185]\n",
      "2000 3.2208328e-07 [0.9996328] [1.1013256]\n"
     ]
    }
   ],
   "source": [
    "# Full code with PlaceHolders\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "w = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bise')\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W + b\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "#minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the Line with new training data\n",
    "for step in range(2001):\n",
    "    #sess.run(train)\n",
    "    cost_val, W_val, b_val, _=sess.run([cost, W, b, train],\n",
    "        feed_dict = {X:[1,2,3,4,5],Y:[2.1,3.1,4.1,5.1,6.1]})\n",
    "    if step % 20 == 0:\n",
    "        #print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
    "        print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T02:45:35.789851Z",
     "start_time": "2020-02-06T02:45:35.781300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.099489]\n",
      "[3.6004076]\n",
      "[2.6007748 4.6000404]\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "print(sess.run(hypothesis, feed_dict={X:[5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X:[1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient(경사) descent(하강) algorithm(알고리즘)\n",
    "* Minimaze cost function\n",
    "* Gradient descent is used many minimization problems\n",
    "* For a given cost function, cost(W, b), it will find W, b to minimize cost\n",
    "* It can be applied to more general function:\n",
    "    cost (w1, w2, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### How it works?\n",
    " \n",
    " * Start with initial guesses\n",
    "     - strat at 0, 0(or any other value)\n",
    "     - Keeping changing W and b a little bit to try and reduce cost(W, b)\n",
    " * Each time you change the parameters, you select the gradient which reduces cost(W, b) the most possible\n",
    " * Repeat\n",
    " * Do so until you converge to a local minimum\n",
    " * Has an intersting property\n",
    "     - Where you start can determine which minimum you end up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T04:05:08.568724Z",
     "start_time": "2020-02-06T04:05:08.232855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X and Y data\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the Line\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "    \n",
    "# Show the cost function\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T04:39:08.240672Z",
     "start_time": "2020-02-06T04:39:08.225157Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'asign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b76155564f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'asign'"
     ]
    }
   ],
   "source": [
    "# Minimaze: Gradient Descent using derivative : W -= Learning_rate * derivative\n",
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T06:07:57.993601Z",
     "start_time": "2020-02-24T06:07:57.832996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.009119 [0.29243276]\n",
      "1 1.9937056 [0.6226308]\n",
      "2 0.5670984 [0.79873645]\n",
      "3 0.16130799 [0.8926594]\n",
      "4 0.04588312 [0.9427517]\n",
      "5 0.013051212 [0.9694676]\n",
      "6 0.003712323 [0.9837161]\n",
      "7 0.0010559557 [0.99131525]\n",
      "8 0.00030035665 [0.9953681]\n",
      "9 8.543671e-05 [0.9975296]\n",
      "10 2.4303998e-05 [0.99868244]\n",
      "11 6.9128632e-06 [0.9992973]\n",
      "12 1.9658282e-06 [0.99962527]\n",
      "13 5.5925625e-07 [0.99980015]\n",
      "14 1.5900957e-07 [0.9998934]\n",
      "15 4.5308028e-08 [0.99994314]\n",
      "16 1.2897022e-08 [0.99996966]\n",
      "17 3.6470347e-09 [0.99998385]\n",
      "18 1.0488357e-09 [0.99999136]\n",
      "19 2.9654146e-10 [0.9999954]\n",
      "20 8.4487084e-11 [0.99999756]\n",
      "21 2.3149482e-11 [0.9999987]\n",
      "22 7.1622708e-12 [0.9999993]\n",
      "23 1.5489832e-12 [0.99999964]\n",
      "24 3.872458e-13 [0.9999998]\n",
      "25 2.9842795e-13 [0.9999999]\n",
      "26 7.460699e-14 [0.99999994]\n",
      "27 0.0 [1.]\n",
      "28 0.0 [1.]\n",
      "29 0.0 [1.]\n",
      "30 0.0 [1.]\n"
     ]
    }
   ],
   "source": [
    "# PlaceHolders\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for Linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimaze: Gradient Descent using derivative : W -= Learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Minimize : Gradient Descent Magic\n",
    "#optomizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "#train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the Line\n",
    "for step in range(31):\n",
    "    sess.run(update, feed_dict={X : x_data, Y: y_data})\n",
    "    print(step, sess.run(cost, feed_dict = {X : x_data, Y : y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T04:50:46.638482Z",
     "start_time": "2020-02-06T04:50:46.400087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -5.0\n",
      "1 -3.3200002\n",
      "2 -2.1104002\n",
      "3 -1.2394881\n",
      "4 -0.61243147\n",
      "5 -0.16095066\n",
      "6 0.16411552\n",
      "7 0.39816317\n",
      "8 0.56667745\n",
      "9 0.6880078\n",
      "10 0.7753656\n",
      "11 0.8382632\n",
      "12 0.8835495\n",
      "13 0.91615564\n",
      "14 0.93963206\n",
      "15 0.9565351\n",
      "16 0.9687053\n",
      "17 0.97746783\n",
      "18 0.9837768\n",
      "19 0.9883193\n",
      "20 0.9915899\n",
      "21 0.99394476\n",
      "22 0.9956402\n",
      "23 0.996861\n",
      "24 0.9977399\n",
      "25 0.99837273\n",
      "26 0.99882835\n",
      "27 0.9991564\n",
      "28 0.9993926\n",
      "29 0.9995627\n",
      "30 0.9996851\n",
      "31 0.99977326\n",
      "32 0.99983674\n",
      "33 0.99988246\n",
      "34 0.99991536\n",
      "35 0.9999391\n",
      "36 0.99995613\n",
      "37 0.9999684\n",
      "38 0.9999773\n",
      "39 0.99998367\n",
      "40 0.99998826\n",
      "41 0.99999154\n",
      "42 0.9999939\n",
      "43 0.9999956\n",
      "44 0.99999684\n",
      "45 0.99999774\n",
      "46 0.9999984\n",
      "47 0.9999988\n",
      "48 0.99999917\n",
      "49 0.9999994\n",
      "50 0.9999996\n",
      "51 0.9999997\n",
      "52 0.99999976\n",
      "53 0.9999998\n",
      "54 0.9999999\n",
      "55 0.99999994\n",
      "56 0.99999994\n",
      "57 0.99999994\n",
      "58 0.99999994\n",
      "59 0.99999994\n",
      "60 0.99999994\n",
      "61 0.99999994\n",
      "62 0.99999994\n",
      "63 0.99999994\n",
      "64 0.99999994\n",
      "65 0.99999994\n",
      "66 0.99999994\n",
      "67 0.99999994\n",
      "68 0.99999994\n",
      "69 0.99999994\n",
      "70 0.99999994\n",
      "71 0.99999994\n",
      "72 0.99999994\n",
      "73 0.99999994\n",
      "74 0.99999994\n",
      "75 0.99999994\n",
      "76 0.99999994\n",
      "77 0.99999994\n",
      "78 0.99999994\n",
      "79 0.99999994\n",
      "80 0.99999994\n",
      "81 0.99999994\n",
      "82 0.99999994\n",
      "83 0.99999994\n",
      "84 0.99999994\n",
      "85 0.99999994\n",
      "86 0.99999994\n",
      "87 0.99999994\n",
      "88 0.99999994\n",
      "89 0.99999994\n",
      "90 0.99999994\n",
      "91 0.99999994\n",
      "92 0.99999994\n",
      "93 0.99999994\n",
      "94 0.99999994\n",
      "95 0.99999994\n",
      "96 0.99999994\n",
      "97 0.99999994\n",
      "98 0.99999994\n",
      "99 0.99999994\n",
      "100 0.99999994\n",
      "101 0.99999994\n",
      "102 0.99999994\n",
      "103 0.99999994\n",
      "104 0.99999994\n",
      "105 0.99999994\n",
      "106 0.99999994\n",
      "107 0.99999994\n",
      "108 0.99999994\n",
      "109 0.99999994\n",
      "110 0.99999994\n",
      "111 0.99999994\n",
      "112 0.99999994\n",
      "113 0.99999994\n",
      "114 0.99999994\n",
      "115 0.99999994\n",
      "116 0.99999994\n",
      "117 0.99999994\n",
      "118 0.99999994\n",
      "119 0.99999994\n",
      "120 0.99999994\n",
      "121 0.99999994\n",
      "122 0.99999994\n",
      "123 0.99999994\n",
      "124 0.99999994\n",
      "125 0.99999994\n",
      "126 0.99999994\n",
      "127 0.99999994\n",
      "128 0.99999994\n",
      "129 0.99999994\n",
      "130 0.99999994\n",
      "131 0.99999994\n",
      "132 0.99999994\n",
      "133 0.99999994\n",
      "134 0.99999994\n",
      "135 0.99999994\n",
      "136 0.99999994\n",
      "137 0.99999994\n",
      "138 0.99999994\n",
      "139 0.99999994\n",
      "140 0.99999994\n",
      "141 0.99999994\n",
      "142 0.99999994\n",
      "143 0.99999994\n",
      "144 0.99999994\n",
      "145 0.99999994\n",
      "146 0.99999994\n",
      "147 0.99999994\n",
      "148 0.99999994\n",
      "149 0.99999994\n",
      "150 0.99999994\n",
      "151 0.99999994\n",
      "152 0.99999994\n",
      "153 0.99999994\n",
      "154 0.99999994\n",
      "155 0.99999994\n",
      "156 0.99999994\n",
      "157 0.99999994\n",
      "158 0.99999994\n",
      "159 0.99999994\n",
      "160 0.99999994\n",
      "161 0.99999994\n",
      "162 0.99999994\n",
      "163 0.99999994\n",
      "164 0.99999994\n",
      "165 0.99999994\n",
      "166 0.99999994\n",
      "167 0.99999994\n",
      "168 0.99999994\n",
      "169 0.99999994\n",
      "170 0.99999994\n",
      "171 0.99999994\n",
      "172 0.99999994\n",
      "173 0.99999994\n",
      "174 0.99999994\n",
      "175 0.99999994\n",
      "176 0.99999994\n",
      "177 0.99999994\n",
      "178 0.99999994\n",
      "179 0.99999994\n",
      "180 0.99999994\n",
      "181 0.99999994\n",
      "182 0.99999994\n",
      "183 0.99999994\n",
      "184 0.99999994\n",
      "185 0.99999994\n",
      "186 0.99999994\n",
      "187 0.99999994\n",
      "188 0.99999994\n",
      "189 0.99999994\n",
      "190 0.99999994\n",
      "191 0.99999994\n",
      "192 0.99999994\n",
      "193 0.99999994\n",
      "194 0.99999994\n",
      "195 0.99999994\n",
      "196 0.99999994\n",
      "197 0.99999994\n",
      "198 0.99999994\n",
      "199 0.99999994\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(-5.0)\n",
    "\n",
    "# Our hypothesis for Linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize : Gradient Descent Magic\n",
    "optomizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the Line\n",
    "for step in range(200):\n",
    "    print(step, sess.run(W))\n",
    "    sess.run(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Multivariable linear refression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T06:15:36.317336Z",
     "start_time": "2020-02-24T06:15:33.348028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost:  275897.3 \n",
      " prediction: \n",
      " [-57.100746 -65.86938  -66.363045 -71.700356 -50.04305 ]\n",
      "10 cost:  inf \n",
      " prediction: \n",
      " [-5.5278927e+35 -6.6441141e+35 -6.5465593e+35 -7.1290396e+35\n",
      " -5.0677862e+35]\n",
      "20 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "30 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "40 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "50 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "60 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "70 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "80 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "90 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "100 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "110 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "120 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "130 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "140 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "150 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "160 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "170 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "180 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "190 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "200 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "210 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "220 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "230 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "240 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "250 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "260 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "270 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "280 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "290 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "300 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "310 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "320 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "330 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "340 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "350 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "360 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "370 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "380 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "390 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "400 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "410 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "420 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "430 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "440 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "450 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "460 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "470 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "480 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "490 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "500 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "510 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "520 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "530 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "540 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "550 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "560 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "570 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "580 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "590 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "600 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "610 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "620 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "630 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "640 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "650 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "660 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "670 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "680 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "690 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "700 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "710 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "720 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "730 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "740 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "750 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "760 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "770 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "780 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "790 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "800 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "810 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "820 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "830 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "840 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "850 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "860 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "870 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "880 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "890 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "900 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "910 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "920 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "930 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "940 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "950 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "960 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "970 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "980 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "990 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1000 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1010 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1020 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1030 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1040 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1050 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1060 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1070 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1080 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1090 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1100 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1110 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1120 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1130 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1140 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1150 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1160 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1170 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1180 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1190 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1200 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1210 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1220 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1230 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1240 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1250 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1260 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1270 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1280 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1290 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1300 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1310 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1320 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1330 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1340 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1350 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1360 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1370 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1380 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1390 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1400 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1410 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1420 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1430 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1440 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1450 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1460 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1470 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1480 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1490 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1500 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1510 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1520 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1530 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1540 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1550 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1560 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1570 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1590 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1600 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1610 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1620 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1630 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1640 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1650 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1660 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1670 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1680 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1690 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1700 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1710 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1720 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1730 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1740 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1750 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1760 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1770 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1780 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1790 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1800 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1810 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1820 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1830 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1840 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1850 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1860 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1870 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1880 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1890 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1900 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1910 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1920 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1930 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1940 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1950 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1960 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1970 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1980 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "1990 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n",
      "2000 cost:  nan \n",
      " prediction: \n",
      " [nan nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# placeholers for a tensor that will be always fed.\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name = 'weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name = 'weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name = 'weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize : Gradient Descent Magic\n",
    "optomizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"cost: \", cost_val, \"\\n prediction: \\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-24T06:15:24.601460Z",
     "start_time": "2020-02-24T06:15:24.451502Z"
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_28' with dtype float\n\t [[Node: Placeholder_28 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_28', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-676b86cb03c1>\", line 11, in <module>\n    Y = tf.placeholder(tf.float32)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1680, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3141, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_28' with dtype float\n\t [[Node: Placeholder_28 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_28' with dtype float\n\t [[Node: Placeholder_28 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c87f0ea1f2bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     cost_val, hy_val, _ = sess.run(\n\u001b[0;32m---> 28\u001b[0;31m         [cost, hypothesis, train], feed_dict={x: x_data, y: y_data})\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cost: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n prediction: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_28' with dtype float\n\t [[Node: Placeholder_28 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder_28', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/sumjack/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-676b86cb03c1>\", line 11, in <module>\n    Y = tf.placeholder(tf.float32)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1680, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3141, in _placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_28' with dtype float\n\t [[Node: Placeholder_28 = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[73., 80., 75.],[93., 88., 93.],[89., 91.,90.],[96., 98., 100.],[73.,66.,70]]\n",
    "y_data = [[152.], [185.], [180.], [196.], [142.]]\n",
    "\n",
    "# placeholers for a tensor that will be always fed.\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w = tf.Variable(tf.random_normal([3, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "hypothesis = tf.matmul(x, w) + b\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize : Gradient Descent Magic\n",
    "optomizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "#Initializes gLobaL vaiables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={x: x_data, y: y_data})\n",
    "    if step % 10 == 0:\n",
    "        print(step, \"cost: \", cost_val, \"\\n prediction: \\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T02:40:50.946267Z",
     "start_time": "2020-02-25T02:40:50.886727Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data = [[1,2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T03:02:26.513678Z",
     "start_time": "2020-02-25T03:02:11.645036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.1886454\n",
      "200 0.99161226\n",
      "400 0.7785861\n",
      "600 0.681765\n",
      "800 0.62895447\n",
      "1000 0.59358877\n",
      "1200 0.5660982\n",
      "1400 0.54267514\n",
      "1600 0.52165085\n",
      "1800 0.5022368\n",
      "2000 0.48403597\n",
      "2200 0.46683708\n",
      "2400 0.45051968\n",
      "2600 0.435009\n",
      "2800 0.42025328\n",
      "3000 0.40621248\n",
      "3200 0.39285207\n",
      "3400 0.38014054\n",
      "3600 0.36804757\n",
      "3800 0.35654363\n",
      "4000 0.3455999\n",
      "4200 0.33518812\n",
      "4400 0.32528087\n",
      "4600 0.31585145\n",
      "4800 0.3068738\n",
      "5000 0.29832336\n",
      "5200 0.29017654\n",
      "5400 0.28241062\n",
      "5600 0.27500427\n",
      "5800 0.26793692\n",
      "6000 0.2611895\n",
      "6200 0.25474378\n",
      "6400 0.24858272\n",
      "6600 0.24269032\n",
      "6800 0.23705135\n",
      "7000 0.23165165\n",
      "7200 0.2264779\n",
      "7400 0.22151764\n",
      "7600 0.21675913\n",
      "7800 0.21219136\n",
      "8000 0.2078042\n",
      "8200 0.20358782\n",
      "8400 0.19953322\n",
      "8600 0.19563197\n",
      "8800 0.19187595\n",
      "9000 0.18825789\n",
      "9200 0.18477066\n",
      "9400 0.18140775\n",
      "9600 0.17816304\n",
      "9800 0.17503071\n",
      "10000 0.17200518\n",
      "\n",
      "Hypothesis: [[0.04074367]\n",
      " [0.17052335]\n",
      " [0.34963843]\n",
      " [0.7616087 ]\n",
      " [0.9264395 ]\n",
      " [0.9758671 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X : x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict= {X: x_data, Y: y_data})\n",
    "    \n",
    "print(\"\\nHypothesis:\", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T02:08:48.959188Z",
     "start_time": "2020-03-10T02:08:37.226266Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.76530993 [[-0.6285459]\n",
      " [-1.1614517]]\n",
      "100 0.7245858 [[-0.5319566 ]\n",
      " [-0.81958133]]\n",
      "200 0.7077319 [[-0.38609746]\n",
      " [-0.5401327 ]]\n",
      "300 0.6998356 [[-0.2732744 ]\n",
      " [-0.35564506]]\n",
      "400 0.6961991 [[-0.19081852]\n",
      " [-0.23484094]]\n",
      "500 0.6945373 [[-0.1320716 ]\n",
      " [-0.15559328]]\n",
      "600 0.69378 [[-0.09085397]\n",
      " [-0.10342061]]\n",
      "700 0.6934353 [[-0.06222456]\n",
      " [-0.06893802]]\n",
      "800 0.6932783 [[-0.04247705]\n",
      " [-0.0460635 ]]\n",
      "900 0.6932069 [[-0.02892448]\n",
      " [-0.03084044]]\n",
      "1000 0.69317436 [[-0.01965832]\n",
      " [-0.02068185]]\n",
      "1100 0.6931596 [[-0.01334085]\n",
      " [-0.01388763]]\n",
      "1200 0.6931528 [[-0.00904312]\n",
      " [-0.00933522]]\n",
      "1300 0.69314975 [[-0.00612435]\n",
      " [-0.00628039]]\n",
      "1400 0.6931484 [[-0.0041447 ]\n",
      " [-0.00422804]]\n",
      "1500 0.69314766 [[-0.0028034 ]\n",
      " [-0.00284793]]\n",
      "1600 0.6931474 [[-0.00189533]\n",
      " [-0.00191911]]\n",
      "1700 0.6931473 [[-0.00128096]\n",
      " [-0.00129367]]\n",
      "1800 0.6931472 [[-0.00086551]\n",
      " [-0.0008723 ]]\n",
      "1900 0.6931472 [[-0.00058468]\n",
      " [-0.00058831]]\n",
      "2000 0.6931472 [[-0.00039489]\n",
      " [-0.00039683]]\n",
      "2100 0.6931472 [[-0.00026671]\n",
      " [-0.00026774]]\n",
      "2200 0.6931472 [[-0.00018009]\n",
      " [-0.00018065]]\n",
      "2300 0.6931472 [[-0.00012159]\n",
      " [-0.00012189]]\n",
      "2400 0.6931472 [[-8.208335e-05]\n",
      " [-8.224222e-05]]\n",
      "2500 0.6931472 [[-5.5410281e-05]\n",
      " [-5.5496115e-05]]\n",
      "2600 0.6931472 [[-3.740223e-05]\n",
      " [-3.744784e-05]]\n",
      "2700 0.6931472 [[-2.5241390e-05]\n",
      " [-2.5270603e-05]]\n",
      "2800 0.6931472 [[-1.7036813e-05]\n",
      " [-1.7051125e-05]]\n",
      "2900 0.6931472 [[-1.14920895e-05]\n",
      " [-1.15019320e-05]]\n",
      "3000 0.6931472 [[-7.750410e-06]\n",
      " [-7.755782e-06]]\n",
      "3100 0.6931472 [[-5.2276437e-06]\n",
      " [-5.2300347e-06]]\n",
      "3200 0.6931472 [[-3.5333778e-06]\n",
      " [-3.5342791e-06]]\n",
      "3300 0.6931472 [[-2.3844980e-06]\n",
      " [-2.3853993e-06]]\n",
      "3400 0.6931472 [[-1.5872823e-06]\n",
      " [-1.5881836e-06]]\n",
      "3500 0.6931472 [[-1.0985203e-06]\n",
      " [-1.0994216e-06]]\n",
      "3600 0.69314724 [[-7.334400e-07]\n",
      " [-7.343413e-07]]\n",
      "3700 0.6931472 [[-4.816103e-07]\n",
      " [-4.825116e-07]]\n",
      "3800 0.6931472 [[-3.087562e-07]\n",
      " [-3.096575e-07]]\n",
      "3900 0.6931472 [[-2.2381926e-07]\n",
      " [-2.2472057e-07]]\n",
      "4000 0.6931472 [[-1.7315512e-07]\n",
      " [-1.7405642e-07]]\n",
      "4100 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4200 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4300 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4400 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4500 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4600 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4700 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4800 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "4900 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5000 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5100 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5200 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5300 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5400 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5500 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5600 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5700 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5800 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "5900 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6000 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6100 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6200 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6300 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6400 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6500 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6600 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6700 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6800 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "6900 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7000 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7100 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7200 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7300 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7400 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7500 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7600 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7700 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7800 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "7900 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8000 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8100 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8200 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8300 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8400 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8500 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8600 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8700 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8800 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "8900 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9000 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9100 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9200 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9300 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9400 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9500 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9600 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9700 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9800 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "9900 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "10000 0.6931472 [[-1.3292183e-07]\n",
      " [-1.3382314e-07]]\n",
      "\n",
      "Hypothesis: [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy :  0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X : x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X : x_data, Y : y_data}), sess.run(W))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict= {X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis:\", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T01:55:27.001986Z",
     "start_time": "2020-03-11T01:55:10.747342Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.792322 [array([[ 1.6111429 , -0.8311001 ],\n",
      "       [-0.8964359 , -0.27018827]], dtype=float32), array([[-0.65419716],\n",
      "       [ 0.5283318 ]], dtype=float32)]\n",
      "200 0.6952596 [array([[ 1.601494  , -0.73541355],\n",
      "       [-0.8530075 , -0.14213707]], dtype=float32), array([[-0.20202257],\n",
      "       [ 0.64851356]], dtype=float32)]\n",
      "400 0.6940806 [array([[ 1.6180606 , -0.69910455],\n",
      "       [-0.82261276, -0.06847222]], dtype=float32), array([[-0.10800723],\n",
      "       [ 0.5941042 ]], dtype=float32)]\n",
      "600 0.6935323 [array([[ 1.6273445 , -0.67877346],\n",
      "       [-0.80912095, -0.00659937]], dtype=float32), array([[-0.04533681],\n",
      "       [ 0.5588496 ]], dtype=float32)]\n",
      "800 0.6932262 [array([[ 1.6301745 , -0.6694655 ],\n",
      "       [-0.80543864,  0.04749228]], dtype=float32), array([[0.00063248],\n",
      "       [0.5367245 ]], dtype=float32)]\n",
      "1000 0.69300264 [array([[ 1.6274174 , -0.66860074],\n",
      "       [-0.8088707 ,  0.09647702]], dtype=float32), array([[0.03890285],\n",
      "       [0.5239968 ]], dtype=float32)]\n",
      "1200 0.6927769 [array([[ 1.6194426 , -0.67495793],\n",
      "       [-0.81910473,  0.14234543]], dtype=float32), array([[0.07595865],\n",
      "       [0.5185539 ]], dtype=float32)]\n",
      "1400 0.6924758 [array([[ 1.6060653 , -0.6882236 ],\n",
      "       [-0.8376587 ,  0.18678546]], dtype=float32), array([[0.1174406],\n",
      "       [0.5194294]], dtype=float32)]\n",
      "1600 0.6919911 [array([[ 1.5866479, -0.7088251],\n",
      "       [-0.8681815,  0.2314761]], dtype=float32), array([[0.1695114],\n",
      "       [0.5266241]], dtype=float32)]\n",
      "1800 0.6911114 [array([[ 1.5605104 , -0.7379434 ],\n",
      "       [-0.9174722 ,  0.27840865]], dtype=float32), array([[0.24040057],\n",
      "       [0.5411949 ]], dtype=float32)]\n",
      "2000 0.6893871 [array([[ 1.5284824 , -0.7776819 ],\n",
      "       [-0.996987  ,  0.33036983]], dtype=float32), array([[0.3423345],\n",
      "       [0.5656976]], dtype=float32)]\n",
      "2200 0.6859182 [array([[ 1.4978616 , -0.83138704],\n",
      "       [-1.1230602 ,  0.3917644 ]], dtype=float32), array([[0.49276942],\n",
      "       [0.60511184]], dtype=float32)]\n",
      "2400 0.6792643 [array([[ 1.4924755 , -0.90428966],\n",
      "       [-1.3110286 ,  0.46980646]], dtype=float32), array([[0.71065694],\n",
      "       [0.66809666]], dtype=float32)]\n",
      "2600 0.6675739 [array([[ 1.555947 , -1.0052627],\n",
      "       [-1.5647211,  0.5757343]], dtype=float32), array([[1.0064218 ],\n",
      "       [0.76809675]], dtype=float32)]\n",
      "2800 0.6479017 [array([[ 1.7276584, -1.150371 ],\n",
      "       [-1.8790246,  0.7266942]], dtype=float32), array([[1.3802568],\n",
      "       [0.9263169]], dtype=float32)]\n",
      "3000 0.61515874 [array([[ 2.0141416, -1.3658562],\n",
      "       [-2.2489843,  0.9494398]], dtype=float32), array([[1.829476 ],\n",
      "       [1.1783032]], dtype=float32)]\n",
      "3200 0.5626665 [array([[ 2.3877409, -1.6833459],\n",
      "       [-2.6617334,  1.2778611]], dtype=float32), array([[2.346062 ],\n",
      "       [1.5725816]], dtype=float32)]\n",
      "3400 0.48578787 [array([[ 2.8042605, -2.1147711],\n",
      "       [-3.088125 ,  1.7249209]], dtype=float32), array([[2.9121687],\n",
      "       [2.1397367]], dtype=float32)]\n",
      "3600 0.39197886 [array([[ 3.2182243, -2.6151648],\n",
      "       [-3.492295 ,  2.2423167]], dtype=float32), array([[3.4994817],\n",
      "       [2.8396688]], dtype=float32)]\n",
      "3800 0.30221328 [array([[ 3.592307 , -3.0999434],\n",
      "       [-3.847888 ,  2.7427611]], dtype=float32), array([[4.0715632],\n",
      "       [3.566498 ]], dtype=float32)]\n",
      "4000 0.23139617 [array([[ 3.9084694, -3.514118 ],\n",
      "       [-4.146375 ,  3.1700602]], dtype=float32), array([[4.5979366],\n",
      "       [4.231267 ]], dtype=float32)]\n",
      "4200 0.18046747 [array([[ 4.167805 , -3.8496394],\n",
      "       [-4.392609 ,  3.515849 ]], dtype=float32), array([[5.0654197],\n",
      "       [4.803127 ]], dtype=float32)]\n",
      "4400 0.14456075 [array([[ 4.380025 , -4.1194477],\n",
      "       [-4.5961986,  3.793512 ]], dtype=float32), array([[5.474831],\n",
      "       [5.286953]], dtype=float32)]\n",
      "4600 0.118886046 [array([[ 4.555626 , -4.3390512],\n",
      "       [-4.7664433,  4.01918  ]], dtype=float32), array([[5.8331213],\n",
      "       [5.697995 ]], dtype=float32)]\n",
      "4800 0.10005282 [array([[ 4.7032046, -4.521021 ],\n",
      "       [-4.9108534,  4.2059207]], dtype=float32), array([[6.148426 ],\n",
      "       [6.0512347]], dtype=float32)]\n",
      "5000 0.08585563 [array([[ 4.8292117, -4.674554 ],\n",
      "       [-5.035103 ,  4.3632917]], dtype=float32), array([[6.4281325],\n",
      "       [6.3587866]], dtype=float32)]\n",
      "5200 0.07487676 [array([[ 4.9383726, -4.80624  ],\n",
      "       [-5.143416 ,  4.498131 ]], dtype=float32), array([[6.6783776],\n",
      "       [6.6299014]], dtype=float32)]\n",
      "5400 0.06619145 [array([[ 5.034164 , -4.9208317],\n",
      "       [-5.238948 ,  4.6153507]], dtype=float32), array([[6.9040966],\n",
      "       [6.871582 ]], dtype=float32)]\n",
      "5600 0.0591825 [array([[ 5.119171 , -5.0217905],\n",
      "       [-5.324079 ,  4.7185407]], dtype=float32), array([[7.109229 ],\n",
      "       [7.0891476]], dtype=float32)]\n",
      "5800 0.053427745 [array([[ 5.195345 , -5.1116986],\n",
      "       [-5.4006195,  4.8103604]], dtype=float32), array([[7.2969255],\n",
      "       [7.2866774]], dtype=float32)]\n",
      "6000 0.048631012 [array([[ 5.264181 , -5.1925063],\n",
      "       [-5.469986 ,  4.8928337]], dtype=float32), array([[7.469714 ],\n",
      "       [7.4673576]], dtype=float32)]\n",
      "6200 0.044579916 [array([[ 5.3268466, -5.2657213],\n",
      "       [-5.5332828,  4.9675045]], dtype=float32), array([[7.629644],\n",
      "       [7.633697]], dtype=float32)]\n",
      "6400 0.04111878 [array([[ 5.3842645, -5.3325224],\n",
      "       [-5.591394 ,  5.035594 ]], dtype=float32), array([[7.77839 ],\n",
      "       [7.787707]], dtype=float32)]\n",
      "6600 0.03813153 [array([[ 5.437176 , -5.3938465],\n",
      "       [-5.6450334,  5.0980644]], dtype=float32), array([[7.9173384],\n",
      "       [7.9310193]], dtype=float32)]\n",
      "6800 0.035529792 [array([[ 5.4861784, -5.4504514],\n",
      "       [-5.6947837,  5.1556945]], dtype=float32), array([[8.047641],\n",
      "       [8.064964]], dtype=float32)]\n",
      "7000 0.03324555 [array([[ 5.531767 , -5.5029507],\n",
      "       [-5.741128 ,  5.209118 ]], dtype=float32), array([[8.170265],\n",
      "       [8.190653]], dtype=float32)]\n",
      "7200 0.031225532 [array([[ 5.5743504, -5.551853 ],\n",
      "       [-5.7844615,  5.258856 ]], dtype=float32), array([[8.286026],\n",
      "       [8.309014]], dtype=float32)]\n",
      "7400 0.02942745 [array([[ 5.614273 , -5.597581 ],\n",
      "       [-5.825125 ,  5.3053427]], dtype=float32), array([[8.395632],\n",
      "       [8.420832]], dtype=float32)]\n",
      "7600 0.027817551 [array([[ 5.651822 , -5.6404905],\n",
      "       [-5.863403 ,  5.348944 ]], dtype=float32), array([[8.499674],\n",
      "       [8.526771]], dtype=float32)]\n",
      "7800 0.026368394 [array([[ 5.687243 , -5.6808805],\n",
      "       [-5.8995385,  5.3899693]], dtype=float32), array([[8.598679],\n",
      "       [8.627403]], dtype=float32)]\n",
      "8000 0.02505761 [array([[ 5.720748, -5.71901 ],\n",
      "       [-5.933741,  5.428682]], dtype=float32), array([[8.6930895],\n",
      "       [8.723226 ]], dtype=float32)]\n",
      "8200 0.023866665 [array([[ 5.752521 , -5.7550983],\n",
      "       [-5.9661927,  5.465308 ]], dtype=float32), array([[8.783307],\n",
      "       [8.814661]], dtype=float32)]\n",
      "8400 0.022780202 [array([[ 5.7827177, -5.789338 ],\n",
      "       [-5.9970517,  5.5000463]], dtype=float32), array([[8.869675],\n",
      "       [8.902088]], dtype=float32)]\n",
      "8600 0.021785252 [array([[ 5.811478 , -5.8218966],\n",
      "       [-6.0264554,  5.5330663]], dtype=float32), array([[8.9525  ],\n",
      "       [8.985835]], dtype=float32)]\n",
      "8800 0.020870944 [array([[ 5.838922 , -5.85292  ],\n",
      "       [-6.0545263,  5.564517 ]], dtype=float32), array([[9.032055],\n",
      "       [9.066198]], dtype=float32)]\n",
      "9000 0.020028096 [array([[ 5.865158 , -5.882535 ],\n",
      "       [-6.0813713,  5.59453  ]], dtype=float32), array([[9.108584],\n",
      "       [9.14343 ]], dtype=float32)]\n",
      "9200 0.019248722 [array([[ 5.8902817, -5.9108543],\n",
      "       [-6.1070848,  5.623222 ]], dtype=float32), array([[9.182303],\n",
      "       [9.217766]], dtype=float32)]\n",
      "9400 0.018526077 [array([[ 5.9143777, -5.937979 ],\n",
      "       [-6.131755 ,  5.6506944]], dtype=float32), array([[9.253406],\n",
      "       [9.289403]], dtype=float32)]\n",
      "9600 0.017854273 [array([[ 5.9375186, -5.9639997],\n",
      "       [-6.1554537,  5.6770425]], dtype=float32), array([[9.322067],\n",
      "       [9.358535]], dtype=float32)]\n",
      "9800 0.017228186 [array([[ 5.959774 , -5.988996 ],\n",
      "       [-6.178253 ,  5.7023444]], dtype=float32), array([[9.388448],\n",
      "       [9.425326]], dtype=float32)]\n",
      "10000 0.016643446 [array([[ 5.9812064, -6.0130405],\n",
      "       [-6.2002106,  5.726678 ]], dtype=float32), array([[9.45269 ],\n",
      "       [9.489927]], dtype=float32)]\n",
      "\n",
      "Hypothesis: [[0.01946584]\n",
      " [0.9849081 ]\n",
      " [0.9852447 ]\n",
      " [0.01670271]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "#W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "#b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "# Neural Net\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", layer1)\n",
    "\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "    \n",
    "    w2_hist = tf.summary.histogram(\"weights1\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases1\", b2)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X : x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X : x_data, Y : y_data}), sess.run([W1, W2]))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict= {X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis:\", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:19:48.782605Z",
     "start_time": "2020-03-11T04:19:48.777339Z"
    }
   },
   "outputs": [],
   "source": [
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:19:49.819438Z",
     "start_time": "2020-03-11T04:19:49.783910Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('./TaeSik')\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:19:51.475987Z",
     "start_time": "2020-03-11T04:19:51.468552Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-8dacf40e5847>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-8dacf40e5847>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    s, _ =sess.run([summary,oprimizer] feed_dict = feed_dict)\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "s, _ =sess.run([summary,oprimizer] feed_dict = feed_dict) \n",
    "writer.add_summary(s, global_step=100)\n",
    "global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "        \n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # input place holders\n",
    "            self.X = th.placeholder(tf.float32, [None, 784])\n",
    "            # img 28x28x1 (black/white)\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "            \n",
    "            # L1 ImgIn shape = (?, 28, 28, 1)\n",
    "            W1 = tf. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 577.897818,
   "position": {
    "height": "40px",
    "left": "1133.45px",
    "right": "20px",
    "top": "90px",
    "width": "575.17px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
